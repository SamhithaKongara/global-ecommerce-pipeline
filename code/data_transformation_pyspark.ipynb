# PySpark transformations in Databricks

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, round

spark = SparkSession.builder.appName("EcommerceDataTransform").getOrCreate()

# Load raw data from ADLS
df = spark.read.option("header", True).csv("abfss://sales-data@<storageaccount>.dfs.core.windows.net/raw/sales.csv")

# Transformations
df_transformed = df.withColumn("order_date", to_date(col("order_date"), "yyyy-MM-dd")) \
                   .withColumn("sales_amount", round(col("sales_amount").cast("double"), 2)) \
                   .dropna()

# Write to curated zone
df_transformed.write.mode("overwrite").parquet("abfss://sales-data@<storageaccount>.dfs.core.windows.net/curated/sales/")
